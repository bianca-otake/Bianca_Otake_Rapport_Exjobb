
@inproceedings{ward_vehicle_2016,
	title = {Vehicle localization with low cost radar sensors},
	doi = {10.1109/IVS.2016.7535489},
	abstract = {Autonomous vehicles rely on {GPS} aided by motion sensors to localize globally within the road network. However, not all driving surfaces have satellite visibility. Therefore, it is important to augment these systems with localization based on environmental sensing such as cameras, lidar and radar in order to increase reliability and robustness. In this work we look at using radar for localization. Radar sensors are available in compact format devices well suited to automotive applications. Past work on localization using radar in automotive applications has been based on careful sensor modeling and Sequential Monte Carlo, (Particle) filtering. In this work we investigate the use of the Iterative Closest Point, {ICP}, algorithm together with an Extended Kalman filter, {EKF}, for localizing a vehicle equipped with automotive grade radars. Experiments using data acquired on public roads shows that this computationally simpler approach yields sufficiently accurate results on par with more complex methods.},
	eventtitle = {2016 {IEEE} Intelligent Vehicles Symposium ({IV})},
	pages = {864--870},
	booktitle = {2016 {IEEE} Intelligent Vehicles Symposium ({IV})},
	author = {Ward, E. and Folkesson, J.},
	date = {2016-06},
	keywords = {automotive applications, automotive grade radars, autonomous vehicles, compact format devices, environmental sensing, extended Kalman filter, Global Positioning System, {GPS}, iterative closest point, Iterative closest point algorithm, Kalman filters, Laser radar, low cost radar sensors, Monte Carlo methods, motion sensors, nonlinear filters, particle filtering, particle filtering (numerical methods), radar receivers, road vehicle radar, Roads, satellite visibility, Sensors, sequential Monte Carlo filtering, Spaceborne radar, vehicle localization, Vehicles},
	annotation = {Folkessons article, first implementation of radar localization},
	file = {IEEE Xplore Abstract Record:/home/bianca/Zotero/storage/AUV6AVMW/7535489.html:text/html;Full Text:/home/bianca/Zotero/storage/CPSE4XSK/Ward and Folkesson - 2016 - Vehicle localization with low cost radar sensors.pdf:application/pdf},
}

@article{ilci_high_2020,
	title = {High Definition 3D Map Creation Using {GNSS}/{IMU}/{LiDAR} Sensor Integration to Support Autonomous Vehicle Navigation},
	volume = {20},
	issn = {1424-8220},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7039384/},
	doi = {10.3390/s20030899},
	abstract = {Recent developments in sensor technologies such as Global Navigation Satellite Systems ({GNSS}), Inertial Measurement Unit ({IMU}), Light Detection and Ranging ({LiDAR}), radar, and camera have led to emerging state-of-the-art autonomous systems, such as driverless vehicles or {UAS} (Unmanned Airborne Systems) swarms. These technologies necessitate the use of accurate object space information about the physical environment around the platform. This information can be generally provided by the suitable selection of the sensors, including sensor types and capabilities, the number of sensors, and their spatial arrangement. Since all these sensor technologies have different error sources and characteristics, rigorous sensor modeling is needed to eliminate/mitigate errors to obtain an accurate, reliable, and robust integrated solution. Mobile mapping systems are very similar to autonomous vehicles in terms of being able to reconstruct the environment around the platforms. However, they differ a lot in operations and objectives. Mobile mapping vehicles use professional grade sensors, such as geodetic grade {GNSS}, tactical grade {IMU}, mobile {LiDAR}, and metric cameras, and the solution is created in post-processing. In contrast, autonomous vehicles use simple/inexpensive sensors, require real-time operations, and are primarily interested in identifying and tracking moving objects. In this study, the main objective was to assess the performance potential of autonomous vehicle sensor systems to obtain high-definition maps based on only using Velodyne sensor data for creating accurate point clouds. In other words, no other sensor data were considered in this investigation. The results have confirmed that cm-level accuracy can be achieved.},
	number = {3},
	journaltitle = {Sensors (Basel, Switzerland)},
	shortjournal = {Sensors (Basel)},
	author = {Ilci, Veli and Toth, Charles},
	urldate = {2021-01-12},
	date = {2020-02-07},
	pmid = {32046232},
	pmcid = {PMC7039384},
	annotation = {Lidar/{IMU}/{GNSS} approach of the mapping-problem},
	file = {PubMed Central Full Text PDF:/home/bianca/Zotero/storage/4IV3K9FQ/Ilci and Toth - 2020 - High Definition 3D Map Creation Using GNSSIMULiD.pdf:application/pdf},
}

@inproceedings{narula_automotive-radar-based_2020,
	title = {Automotive-Radar-Based 50-cm Urban Positioning},
	doi = {10.1109/PLANS46316.2020.9109917},
	abstract = {Deployment of automated ground vehicles ({AGVs}) beyond the confines of sunny and dry climes will require sub-lane-level positioning techniques based on radio waves rather than near-visible-light radiation. Like human sight, lidar and cameras perform poorly in low-visibility conditions. This paper develops and demonstrates a novel technique for robust 50-cm-accurate urban ground positioning based on commercially-available low-cost automotive radars. The technique is computationally efficient yet obtains a globally-optimal translation and heading solution, avoiding local minima caused by repeating patterns in the urban radar environment. Performance is evaluated on an extensive and realistic urban data set. Comparison against ground truth shows that, when coupled with stable short-term odometry, the technique maintains 95-percentile errors below 50 cm in horizontal position and 1° in heading.},
	eventtitle = {2020 {IEEE}/{ION} Position, Location and Navigation Symposium ({PLANS})},
	pages = {856--867},
	booktitle = {2020 {IEEE}/{ION} Position, Location and Navigation Symposium ({PLANS})},
	author = {Narula, L. and Iannucci, P. A. and Humphreys, T. E.},
	date = {2020-04},
	note = {{ISSN}: 2153-3598},
	keywords = {road vehicle radar, {AGV}, all-weather positioning, automated ground vehicles, automated vehicles, automatic guided vehicles, Automotive radar, automotive radars, cameras, distance 50.0 cm, distance measurement, dry climes, globally-optimal translation, horizontal position, lidar, localization, low-visibility conditions, mobile robots, near-visible-light radiation, path planning, position control, realistic urban data, sub-lane-level positioning techniques, sunny climes, urban ground positioning, urban positioning, urban radar environment},
	annotation = {As a result, this paper only considersradar returns with range less than 50 m.},
	annotation = {Base project on this implementation
 },
	annotation = {Is it a sliding window of collected data or pure batches? },
	file = {IEEE Xplore Abstract Record:/home/bianca/Zotero/storage/CA2LE9U9/9109917.html:text/html;Submitted Version:/home/bianca/Zotero/storage/K2R5BMQN/Narula et al. - 2020 - Automotive-Radar-Based 50-cm Urban Positioning.pdf:application/pdf},
}

@book{thrun_probabilistic_2005,
	location = {Cambridge, Mass},
	title = {Probabilistic robotics},
	isbn = {978-0-262-20162-9},
	series = {Intelligent robotics and autonomous agents},
	pagetotal = {647},
	publisher = {{MIT} Press},
	author = {Thrun, Sebastian and Burgard, Wolfram and Fox, Dieter},
	date = {2005},
	langid = {english},
	note = {{OCLC}: ocm58451645},
	keywords = {Probabilities, Robotics},
	annotation = {Book about {OGMs}
 },
	file = {Thrun et al. - 2005 - Probabilistic robotics.pdf:/home/bianca/Zotero/storage/HVSNQGPV/Thrun et al. - 2005 - Probabilistic robotics.pdf:application/pdf},
}

@inproceedings{yoneda_vehicle_2018,
	title = {Vehicle Localization using 76GHz Omnidirectional Millimeter-Wave Radar for Winter Automated Driving*},
	doi = {10.1109/IVS.2018.8500378},
	abstract = {This paper presents the 76GHz {MWR} (Millimeter-Wave Radar)-based self-localization method for automated driving during snowfall. Previously, there were many {LIDAR} (Light Detection and Ranging)-based localization techniques proposed for high measurement accuracy and robustness to changes of day and night. However, they did not provide effective approaches for snow conditions because of sensing noise (i.e., environmental resistance) created by snowfall. Therefore, this paper developed a {MWR}-based map generation and a real-time localization method by modeling the uncertainties of error propagation. Quantitative evaluations are performed on driving data with and without snow conditions, using a {LIDAR}-based method as the baseline. Experimental results show that a lateral root mean square error of about 0. 25m can be obtained, regardless of the presence or absence of snowfall. Thus, it can be investigated that a potential performance of radar-based localization.},
	eventtitle = {2018 {IEEE} Intelligent Vehicles Symposium ({IV})},
	pages = {971--977},
	booktitle = {2018 {IEEE} Intelligent Vehicles Symposium ({IV})},
	author = {Yoneda, K. and Hashimoto, N. and Yanase, R. and Aldibaja, M. and Suganuma, N.},
	date = {2018-06},
	note = {{ISSN}: 1931-0587},
	keywords = {Laser radar, Roads, Sensors, vehicle localization, Acceleration, Dynamics, environmental resistance, Global navigation satellite system, lateral root mean square error, {LIDAR}-based method, mean square error methods, {MWR}-based map generation, omnidirectional Millimeter-Wave Radar, optical radar, radar-based localization, real-time localization method, robustness, snow, Snow, snowfall, traffic engineering computing, winter automated driving},
	annotation = {Equations on how to filter out moving obstacles!
Makes some kind of pixel probability map.},
	annotation = {Japanese article but no information on how often the localization data is retrieved 
 },
	file = {IEEE Xplore Full Text PDF:/home/bianca/Zotero/storage/5HBFSR5W/Yoneda et al. - 2018 - Vehicle Localization using 76GHz Omnidirectional M.pdf:application/pdf;IEEE Xplore Abstract Record:/home/bianca/Zotero/storage/RPDVIR9R/Yoneda et al. - 2018 - Vehicle Localization using 76GHz Omnidirectional M.html:text/html},
}

@article{hong_radarslam_2020,
	title = {{RadarSLAM}: Radar based Large-Scale {SLAM} in All Weathers},
	url = {http://arxiv.org/abs/2005.02198},
	shorttitle = {{RadarSLAM}},
	abstract = {Numerous Simultaneous Localization and Mapping ({SLAM}) algorithms have been presented in last decade using different sensor modalities. However, robust {SLAM} in extreme weather conditions is still an open research problem. In this paper, {RadarSLAM}, a full radar based graph {SLAM} system, is proposed for reliable localization and mapping in large-scale environments. It is composed of pose tracking, local mapping, loop closure detection and pose graph optimization, enhanced by novel feature matching and probabilistic point cloud generation on radar images. Extensive experiments are conducted on a public radar dataset and several self-collected radar sequences, demonstrating the state-of-the-art reliability and localization accuracy in various adverse weather conditions, such as dark night, dense fog and heavy snowfall.},
	journaltitle = {{arXiv}:2005.02198 [cs]},
	author = {Hong, Ziyang and Petillot, Yvan and Wang, Sen},
	urldate = {2021-01-26},
	date = {2020-05-05},
	eprinttype = {arxiv},
	eprint = {2005.02198},
	keywords = {Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/home/bianca/Zotero/storage/7R3FTAPH/Hong et al. - 2020 - RadarSLAM Radar based Large-Scale SLAM in All Wea.pdf:application/pdf;arXiv.org Snapshot:/home/bianca/Zotero/storage/Z2GUMAQJ/Hong et al. - 2020 - RadarSLAM Radar based Large-Scale SLAM in All Wea.html:text/html},
}

@inproceedings{holder_real-time_2019,
	title = {Real-Time Pose Graph {SLAM} based on Radar},
	doi = {10.1109/IVS.2019.8813841},
	abstract = {This work presents a real-time pose graph based Simultaneous Localization and Mapping ({SLAM}) system for automotive Radar. The algorithm constructs a map from Radar detections using the Iterative Closest Point ({ICP}) method to match consecutive scans obtained from a single, front-facing Radar sensor. The algorithm is evaluated on a range of realworld datasets and shows mean translational errors as low as 0.62 m and demonstrates robustness on long tracks. Using a single Radar, our proposed system achieves state-of-the-art performance when compared to other Radar-based {SLAM} algorithms that use multiple, higher-resolution Radars.},
	eventtitle = {2019 {IEEE} Intelligent Vehicles Symposium ({IV})},
	pages = {1145--1151},
	booktitle = {2019 {IEEE} Intelligent Vehicles Symposium ({IV})},
	author = {Holder, M. and Hellwig, S. and Winner, H.},
	date = {2019-06},
	note = {{ISSN}: 2642-7214},
	keywords = {radar receivers, road vehicle radar, mobile robots, automotive Radar, consecutive scans, front-facing radar sensor, graph theory, higher-resolution Radars, {ICP} method, image resolution, Iterative Closest Point method, iterative methods, Mapping system, pose estimation, radar detection, Radar detections, radar resolution, Radar-based {SLAM} algorithms, real time pose graph {SLAM}, real-time pose graph, simultaneous localization and mapping system, {SLAM} (robots)},
	annotation = {Pose graph approach, using keypoints and {GLARE} signature to match submaps },
	file = {IEEE Xplore Full Text PDF:/home/bianca/Zotero/storage/2Z3GT92C/Holder et al. - 2019 - Real-Time Pose Graph SLAM based on Radar.pdf:application/pdf;IEEE Xplore Abstract Record:/home/bianca/Zotero/storage/BM8KGTEG/Holder et al. - 2019 - Real-Time Pose Graph SLAM based on Radar.html:text/html},
}

@article{cen_radar-only_2019,
	title = {Radar-only ego-motion estimation in difficult settings via graph matching},
	url = {http://arxiv.org/abs/1904.11476},
	abstract = {Radar detects stable, long-range objects under variable weather and lighting conditions, making it a reliable and versatile sensor well suited for ego-motion estimation. In this work, we propose a radar-only odometry pipeline that is highly robust to radar artifacts (e.g., speckle noise and false positives) and requires only one input parameter. We demonstrate its ability to adapt across diverse settings, from urban {UK} to off-road Iceland, achieving a scan matching accuracy of approximately 5.20 cm and 0.0929 deg when using {GPS} as ground truth (compared to visual odometry's 5.77 cm and 0.1032 deg). We present algorithms for keypoint extraction and data association, framing the latter as a graph matching optimization problem, and provide an in-depth system analysis.},
	journaltitle = {{arXiv}:1904.11476 [cs]},
	author = {Cen, Sarah H. and Newman, Paul},
	urldate = {2021-01-27},
	date = {2019-04-25},
	eprinttype = {arxiv},
	eprint = {1904.11476},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition},
	annotation = {Data association by globally optimal (not {ICP}) method and algorithm explained in article.},
	annotation = {Forest in Iceland, algorithms for keypoint extraction and data association!},
	annotation = {Odometry readings obtained at 4Hz frequency },
	file = {arXiv Fulltext PDF:/home/bianca/Zotero/storage/ZF9TXPX4/Cen and Newman - 2019 - Radar-only ego-motion estimation in difficult sett.pdf:application/pdf;arXiv.org Snapshot:/home/bianca/Zotero/storage/5MT6WW7I/Cen and Newman - 2019 - Radar-only ego-motion estimation in difficult sett.html:text/html},
}

@inproceedings{aldera_what_2019,
	title = {What Could Go Wrong? Introspective Radar Odometry in Challenging Environments},
	doi = {10.1109/ITSC.2019.8917111},
	shorttitle = {What Could Go Wrong?},
	abstract = {This paper is about detecting failures under uncertainty and improving the reliability of radar-only motion estimation. We use weak supervision together with inertial measurement fusion to train a classifier that exploits the principal eigenvector associated with our radar scan matching algorithm at run-time and produces a prior belief in the robot's motion estimate. This prior is used in a filtering framework to correct for vehicle motion estimates. We demonstrate the system on a challenging outdoor dataset, for which current radar motion estimation algorithms fail frequently. By knowing when failure is likely, we achieve qualitatively superior motion estimates and quantitatively fewer odometry failures. Specifically, we see 24.7 \% fewer failures in motion estimation over the course of a 15.81 km drive through a difficult, mixed rural-and-urban scene, with lower {RMSE} in translational and rotational estimates during particularly challenging conditions.},
	eventtitle = {2019 {IEEE} Intelligent Transportation Systems Conference ({ITSC})},
	pages = {2835--2842},
	booktitle = {2019 {IEEE} Intelligent Transportation Systems Conference ({ITSC})},
	author = {Aldera, R. and Martini, D. D. and Gadd, M. and Newman, P.},
	date = {2019-10},
	keywords = {Global Positioning System, road vehicle radar, distance measurement, mean square error methods, traffic engineering computing, distance 15.81 km, ego-motion estimation, eigenvalues and eigenfunctions, field robotics, filtering framework, image classification, image filtering, image matching, inertial measurement fusion, introspection, introspective radar odometry, mixed rural-and-urban scene, motion estimation, Motion estimation, principal eigenvector, prior belief, qualitatively superior motion estimates, quantitatively fewer odometry failures, radar, radar imaging, Radar measurements, radar motion estimation algorithms, radar scan matching algorithm, radar-only motion estimation, reliability, {RMSE}, robot motion estimate, Robot sensing systems, Robustness, sensing, sensor fusion, vehicle motion estimates},
	annotation = {Discusses and presents common scenarios where radar odometry ({RO}) fails and why
 },
	file = {IEEE Xplore Full Text PDF:/home/bianca/Zotero/storage/K8XNZ4TG/Aldera et al. - 2019 - What Could Go Wrong Introspective Radar Odometry .pdf:application/pdf;IEEE Xplore Abstract Record:/home/bianca/Zotero/storage/XZEJSRQG/Aldera et al. - 2019 - What Could Go Wrong Introspective Radar Odometry .html:text/html},
}

@inproceedings{cen_precise_2018,
	title = {Precise Ego-Motion Estimation with Millimeter-Wave Radar Under Diverse and Challenging Conditions},
	doi = {10.1109/ICRA.2018.8460687},
	abstract = {In contrast to cameras, lidars, {GPS}, and proprioceptive sensors, radars are affordable and efficient systems that operate well under variable weather and lighting conditions, require no external infrastructure, and detect long-range objects. In this paper, we present a reliable and accurate radar-only motion estimation algorithm for mobile autonomous systems. Using a frequency-modulated continuous-wave ({FMCW}) scanning radar, we first extract landmarks with an algorithm that accounts for unwanted effects in radar returns. To estimate relative motion, we then perform scan matching by greedily adding point correspondences based on unary descriptors and pairwise compatibility scores. Our radar odometry results are robust under a variety of conditions, including those under which visual odometry and {GPS}/{INS} fail.},
	eventtitle = {2018 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	pages = {6045--6052},
	booktitle = {2018 {IEEE} International Conference on Robotics and Automation ({ICRA})},
	author = {Cen, S. H. and Newman, P.},
	date = {2018-05},
	note = {{ISSN}: 2577-087X},
	keywords = {Global Positioning System, Sensors, cameras, distance measurement, image matching, motion estimation, Motion estimation, Robustness, Azimuth, {CW} radar, feature extraction, Feature extraction, {FM} radar, frequency-modulated continuous-wave scanning radar, {GPS}/{INS}, lidars, long-range objects, millimeter-wave radar, millimetre wave radar, mobile autonomous systems, precise ego-motion estimation, proprioceptive sensors, Radar imaging, radar odometry, radars},
	annotation = {Many algorithms of data association and key extraction etc. Radar Odometry articee from Cen
 },
	file = {IEEE Xplore Full Text PDF:/home/bianca/Zotero/storage/CNYBWDR5/Cen and Newman - 2018 - Precise Ego-Motion Estimation with Millimeter-Wave.pdf:application/pdf;IEEE Xplore Abstract Record:/home/bianca/Zotero/storage/95E4BCPB/8460687.html:text/html},
}

@article{ort_autonomous_2020,
	title = {Autonomous Navigation in Inclement Weather Based on a Localizing Ground Penetrating Radar},
	volume = {5},
	issn = {2377-3766},
	doi = {10.1109/LRA.2020.2976310},
	abstract = {Most autonomous driving solutions require some method of localization within their environment. Typically, onboard sensors are used to localize the vehicle precisely in a previously recorded map. However, these solutions are sensitive to ambient lighting conditions such as darkness and inclement weather. Additionally, the maps can become outdated in a rapidly changing environment and require continuous updating. While {LiDAR} systems don't require visible light, they are sensitive to weather such as fog or snow, which can interfere with localization. In this letter, we utilize a Ground Penetrating Radar ({GPR}) to obtain precise vehicle localization. By mapping and localizing using features beneath the ground, we obtain features that are both stable over time, and maintain their appearance during changing ambient weather and lighting conditions. We incorporate this solution into a full-scale autonomous vehicle and evaluate the performance on over 17 km of testing data in a variety of challenging weather conditions. We find that this novel sensing modality is capable of providing precise localization for autonomous navigation without using cameras or {LiDAR} sensors.},
	pages = {3267--3274},
	number = {2},
	journaltitle = {{IEEE} Robotics and Automation Letters},
	author = {Ort, T. and Gilitschenski, I. and Rus, D.},
	date = {2020-04},
	note = {Conference Name: {IEEE} Robotics and Automation Letters},
	keywords = {radar receivers, vehicle localization, cameras, Robot sensing systems, ambient lighting conditions, autonomous driving solutions, Autonomous vehicle navigation, Autonomous vehicles, field robots, full-scale autonomous vehicle navigation, {GPR}, ground penetrating radar, Ground penetrating radar, ground penetrating radar localization, intelligent transportation systems, {LiDAR} sensor system, Lighting, Meteorology, onboard sensors, radionavigation, sensors, Vehicle dynamics, wheeled robots},
	annotation = {Underground! Tests hypothethical poses in a 3D radar map.
 },
	file = {IEEE Xplore Full Text PDF:/home/bianca/Zotero/storage/GF5V8II6/Ort et al. - 2020 - Autonomous Navigation in Inclement Weather Based o.pdf:application/pdf;IEEE Xplore Abstract Record:/home/bianca/Zotero/storage/92GZ2G9P/9013076.html:text/html},
}

@inproceedings{schuster_landmark_2016,
	title = {Landmark based radar {SLAM} using graph optimization},
	doi = {10.1109/ITSC.2016.7795967},
	abstract = {On the way to achieving higher degrees of autonomy for vehicles in complicated, ever changing scenarios, the localization problem poses a very important role. Especially the Simultaneous Localization and Mapping ({SLAM}) problem has been studied greatly in the past. For an autonomous system in the real world, we present a very cost-efficient, robust and very precise localization approach based on {GraphSLAM} and graph optimization using radar sensors. We are able to prove on a dynamically changing parking lot layout that both mapping and localization accuracy are very high. To evaluate the performance of the mapping algorithm, a highly accurate ground truth map generated from a total station was used. Localization results are compared to a high precision {DGPS}/{INS} system. Utilizing these methods, we can show the strong performance of our algorithm.},
	eventtitle = {2016 {IEEE} 19th International Conference on Intelligent Transportation Systems ({ITSC})},
	pages = {2559--2564},
	booktitle = {2016 {IEEE} 19th International Conference on Intelligent Transportation Systems ({ITSC})},
	author = {Schuster, F. and Keller, C. G. and Rapp, M. and Haueis, M. and Curio, C.},
	date = {2016-11},
	note = {{ISSN}: 2153-0017},
	keywords = {Global Positioning System, radar receivers, Vehicles, Radar measurements, Robustness, Feature extraction, graph optimization, high-precision {DGPS}-{INS} system, inertial navigation, landmark-based radar {SLAM} problem, parking lot layout changing, radar sensor, Simultaneous localization and mapping, simultaneous localization and mapping problem},
	annotation = {{RANSAC} Older {SLAM} asrticle! And {SLAM} Approach. Describes the {graphSLAM} approach. And keypoints/Descriptors. They use an "R-tree" for data map storage. Quick to access.},
	file = {IEEE Xplore Full Text PDF:/home/bianca/Zotero/storage/P8WKZ6S4/Schuster et al. - 2016 - Landmark based radar SLAM using graph optimization.pdf:application/pdf;IEEE Xplore Abstract Record:/home/bianca/Zotero/storage/IKVSL236/7795967.html:text/html},
}

@article{guttman_r-trees_nodate,
	title = {R-Trees - A Dynamic Index Structure for Spatial Searching},
	pages = {11},
	author = {Guttman, Antomn},
	langid = {english},
	file = {Guttman - R-Trees - A Dynamic Index Structure for Spatial Se.pdf:/home/bianca/Zotero/storage/EYVN8GE7/Guttman - R-Trees - A Dynamic Index Structure for Spatial Se.pdf:application/pdf},
}

@inproceedings{jurgens_radar-based_2020,
	title = {Radar-based Automotive Localization using Landmarks in a Multimodal Sensor Graph-based Approach},
	doi = {10.23919/IRS48640.2020.9253921},
	abstract = {Highly automated driving functions currently often rely on a-priori knowledge from maps for planning and prediction in complex scenarios like cities. This makes map-relative localization an essential skill. In this paper, we address the problem of localization with automotive-grade radars, using a real-time graph-based {SLAM} approach. The system uses landmarks and odometry information as an abstraction layer. This way, besides radars, all kind of different sensor modalities including cameras and lidars can contribute. A single, semantic landmark map is used and maintained for all sensors. We implemented our approach using C++ and thoroughly tested it on data obtained with our test vehicles, comprising cars and trucks. Test scenarios include inner cities and industrial areas like container terminals. The experiments presented in this paper suggest that the approach is able to provide a precise and stable pose in structured environments, using radar data alone. The fusion of additional sensor information from cameras or lidars further boost performance, providing reliable semantic information needed for automated mapping.},
	eventtitle = {2020 21st International Radar Symposium ({IRS})},
	pages = {373--378},
	booktitle = {2020 21st International Radar Symposium ({IRS})},
	author = {Jürgens, S. and Koch, N. and Meinecke, M.-M.},
	date = {2020-10},
	note = {{ISSN}: 2155-5753},
	keywords = {Laser radar, cameras, optical radar, graph theory, {SLAM} (robots), sensor fusion, a-priori knowledge, automated driving functions, Automobiles, automotive-grade radars, C++ language, Cameras, driver information systems, image fusion, Location awareness, map-relative localization, multimodal sensor graph, odometry information, radar data, radar-based automotive localization, real-time graph-based {SLAM}, Reliability, semantic information, Semantics, test vehicles, Urban areas},
	annotation = {Newer article where 2D automotive radars are used. Collected during 400 ms. Radar data points are removed Maybe not so much useful informations more than a {PoC}.
 },
	annotation = {{SLAM} approach multi sensor fusion. Use lidar, so maybe not 100\% relatable.},
	file = {IEEE Xplore Full Text PDF:/home/bianca/Zotero/storage/VJ45SDFK/Jürgens et al. - 2020 - Radar-based Automotive Localization using Landmark.pdf:application/pdf;IEEE Xplore Abstract Record:/home/bianca/Zotero/storage/5E9UIQSI/9253921.html:text/html},
}

@inproceedings{barjenbruch_joint_2015,
	title = {Joint spatial- and Doppler-based ego-motion estimation for automotive radars},
	doi = {10.1109/IVS.2015.7225789},
	abstract = {An ego-motion estimation method based on the spatial and Doppler information obtained by an automotive radar is proposed. The estimation of the motion state vector is performed in a density-based framework. Compared to standard vehicle odometry the approach is capable to estimate the full two dimensional motion state with three degrees of freedom. The measurement of a Doppler radar sensor is represented as a mixture of Gaussians. This mixture is matched with the mixture of a previous measurement by applying the appropriate egomotion transformation. The parameters of the transformation are found by the optimization of a suitable join metric. Due to the Doppler information the method is very robust against disturbances by moving objects and clutter. It provides excellent results for highly nonlinear movements. Real world results of the proposed method are presented. The measurements are obtained by a 77GHz radar sensor mounted on a test vehicle. A comparison using a high-precision inertial measurement unit with differential {GPS} support is made. The results show a high accuracy in velocity and yaw-rate estimation.},
	eventtitle = {2015 {IEEE} Intelligent Vehicles Symposium ({IV})},
	pages = {839--844},
	booktitle = {2015 {IEEE} Intelligent Vehicles Symposium ({IV})},
	author = {Barjenbruch, M. and Kellner, D. and Klappstein, J. and Dickmann, J. and Dietmayer, K.},
	date = {2015-06},
	note = {{ISSN}: 1931-0587},
	keywords = {Vehicles, automotive radars, traffic engineering computing, motion estimation, radar imaging, driver information systems, automobiles, Current measurement, density-based framework, Doppler effect, Doppler information, Doppler radar sensor measurement, Doppler-based ego-motion estimation, Estimation, Gaussian mixture, motion state vector estimation, Radar, spatial information, spatial-based ego-motion estimation, Standards, vehicle odometry, velocity estimation, yaw-rate estimation},
	annotation = {Expected doppler velocity from sensors! Better than Folkessons description of the algorithm
 },
	file = {IEEE Xplore Full Text PDF:/home/bianca/Zotero/storage/ZD2IKHRX/Barjenbruch et al. - 2015 - Joint spatial- and Doppler-based ego-motion estima.pdf:application/pdf;IEEE Xplore Abstract Record:/home/bianca/Zotero/storage/ILYUMYIR/7225789.html:text/html},
}

@inproceedings{segal_generalized-icp_2009,
	title = {Generalized-{ICP}.},
	isbn = {978-0-262-51463-7},
	url = {http://dblp.uni-trier.de/db/conf/rss/rss2009.html#SegalHT09},
	booktitle = {Robotics: Science and Systems},
	publisher = {The {MIT} Press},
	author = {Segal, Aleksandr and Hähnel, Dirk and Thrun, Sebastian},
	editor = {Trinkle, Jeff and Matsuoka, Yoky},
	date = {2009},
	keywords = {dblp},
	annotation = {{ICP}, generalized {ICP}. About matching point clouds with each other when stored in k-d trees.},
	file = {Generalized_ICP.pdf:/home/bianca/Zotero/storage/BFPIT59A/Generalized_ICP.pdf:application/pdf},
}

@book{biber_probabilistic_2004,
	title = {A Probabilistic Framework for Robust and Accurate Matching of Point Clouds},
	isbn = {978-3-540-22945-2},
	abstract = {Abstract We present a probabilistic framework for matching of point clouds Variants of the {ICP} algorithm typically pair points to points or points to lines Instead, we pair data points to probability functions that are thought of having generated the data points Then an energy function is derived from a maximum likelihood formulation Each such distribu - tion is a mixture of a bivariate Normal Distribution to capture the local structure of points and an explicit outlier term to achieve robustness We apply our approach to the {SLAM} problem in robotics using a 2D laser range scanner},
	pagetotal = {480},
	author = {Biber, Peter and Fleck, Sven and Straßer, Wolfgang},
	date = {2004-08-30},
	doi = {10.1007/978-3-540-28649-3_59},
	note = {Pages: 487},
}

@article{burnett_we_2021,
	title = {Do We Need to Compensate for Motion Distortion and Doppler Effects in Spinning Radar Navigation?},
	volume = {6},
	issn = {2377-3766},
	doi = {10.1109/LRA.2021.3052439},
	abstract = {In order to tackle the challenge of unfavorable weather conditions such as rain and snow, radar is being revisited as a parallel sensing modality to vision and lidar. Recent works have made tremendous progress in applying spinning radar to odometry and place recognition. However, these works have so far ignored the impact of motion distortion and Doppler effects on spinning-radar-based navigation, which may be significant in the self-driving car domain where speeds can be high. In this work, we demonstrate the effect of these distortions on radar odometry using the Oxford Radar {RobotCar} Dataset and metric localization using our own data-taking platform. We revisit a lightweight estimator that can recover the motion between a pair of radar scans while accounting for both effects. Our conclusion is that both motion distortion and the Doppler effect are significant in different aspects of spinning radar navigation, with the former more prominent than the latter. Code for this project can be found at: https://github.com/keenan-burnett/yeti\_radar\_odometry},
	pages = {771--778},
	number = {2},
	journaltitle = {{IEEE} Robotics and Automation Letters},
	author = {Burnett, K. and Schoellig, A. P. and Barfoot, T. D.},
	date = {2021-04},
	note = {Conference Name: {IEEE} Robotics and Automation Letters},
	keywords = {Laser radar, Spaceborne radar, Robot sensing systems, intelligent transportation systems, Radar, Distortion, Doppler radar, Localization, Navigation, range sensing},
	file = {IEEE Xplore Full Text PDF:/home/bianca/Zotero/storage/RS2DV9EZ/Burnett et al. - 2021 - Do We Need to Compensate for Motion Distortion and.pdf:application/pdf;IEEE Xplore Abstract Record:/home/bianca/Zotero/storage/YGCXBNA9/9327473.html:text/html},
}

@article{weston_probably_2019,
	title = {Probably Unknown: Deep Inverse Sensor Modelling In Radar},
	url = {http://arxiv.org/abs/1810.08151},
	shorttitle = {Probably Unknown},
	abstract = {Radar presents a promising alternative to lidar and vision in autonomous vehicle applications, able to detect objects at long range under a variety of weather conditions. However, distinguishing between occupied and free space from raw radar power returns is challenging due to complex interactions between sensor noise and occlusion. To counter this we propose to learn an Inverse Sensor Model ({ISM}) converting a raw radar scan to a grid map of occupancy probabilities using a deep neural network. Our network is self-supervised using partial occupancy labels generated by lidar, allowing a robot to learn about world occupancy from past experience without human supervision. We evaluate our approach on five hours of data recorded in a dynamic urban environment. By accounting for the scene context of each grid cell our model is able to successfully segment the world into occupied and free space, outperforming standard {CFAR} filtering approaches. Additionally by incorporating heteroscedastic uncertainty into our model formulation, we are able to quantify the variance in the uncertainty throughout the sensor observation. Through this mechanism we are able to successfully identify regions of space that are likely to be occluded.},
	journaltitle = {{arXiv}:1810.08151 [cs]},
	author = {Weston, Rob and Cen, Sarah and Newman, Paul and Posner, Ingmar},
	urldate = {2021-02-08},
	date = {2019-05-10},
	eprinttype = {arxiv},
	eprint = {1810.08151},
	keywords = {Computer Science - Robotics},
	annotation = {Comment: 6 full pages, 1 page of references},
	file = {arXiv Fulltext PDF:/home/bianca/Zotero/storage/783GHDA6/Weston et al. - 2019 - Probably Unknown Deep Inverse Sensor Modelling In.pdf:application/pdf;arXiv.org Snapshot:/home/bianca/Zotero/storage/K2H6KWNS/1810.html:text/html},
}

@inproceedings{slutsky_dual_2019,
	title = {Dual Inverse Sensor Model for Radar Occupancy Grids},
	doi = {10.1109/IVS.2019.8813772},
	abstract = {Occupancy grids ({OG}) are widely used for low-level fusion of radar data in various automotive applications. At the core of {OG} generation, usually, there is an inverse sensor model ({ISM}), which is a conditional cell occupancy probability model. Traditional {ISM}'s lack mechanisms decreasing occupancy likelihoods along directions that produce no detections; thus, false detections tend to perpetuate on the {OG}. In this paper, we propose a novel Inverse Sensor Model including a “positive” component describing occupancy probabilities induced by radar detections and a “negative” component handling lack of detections in a given direction. This dual model proves especially useful in multi-sensor/multi-frame context since false detections by different radars and/or at different moments are uncorrelated and thus can be efficiently mitigated.},
	eventtitle = {2019 {IEEE} Intelligent Vehicles Symposium ({IV})},
	pages = {1760--1767},
	booktitle = {2019 {IEEE} Intelligent Vehicles Symposium ({IV})},
	author = {Slutsky, M. and Dobkin, D.},
	date = {2019-06},
	note = {{ISSN}: 2642-7214},
	keywords = {automotive applications, radar detection, Robot sensing systems, sensor fusion, Azimuth, Radar imaging, driver information systems, radar data, automobiles, Doppler radar, collision avoidance, conditional cell occupancy probability model, dual inverse sensor model, false detections, low-level fusion, multiframe context, multisensor context, negative component handling lack, occupancy grids, occupancy probabilities, {OG} generation, positive component, probability, Radar detection, radar detections, radar occupancy, Three-dimensional displays},
	file = {IEEE Xplore Full Text PDF:/home/bianca/Zotero/storage/J3KJVUCX/Slutsky and Dobkin - 2019 - Dual Inverse Sensor Model for Radar Occupancy Grid.pdf:application/pdf;IEEE Xplore Abstract Record:/home/bianca/Zotero/storage/SV68ETIY/8813772.html:text/html},
}

@article{office_79_2015,
	title = {79 {GHz}-Band High-Resolution Millimeter-Wave Radar},
	volume = {51},
	pages = {5},
	number = {4},
	journaltitle = {{FUJITSU} Sci. Tech. J.},
	author = {Office, {FSTJ} Editorial},
	date = {2015},
	langid = {english},
	file = {Office - 2015 - 79 GHz-Band High-Resolution Millimeter-Wave Radar.pdf:/home/bianca/Zotero/storage/D97H84QS/Office - 2015 - 79 GHz-Band High-Resolution Millimeter-Wave Radar.pdf:application/pdf},
}

@article{grisetti_tutorial_2010,
	title = {A Tutorial on Graph-Based {SLAM}},
	volume = {2},
	issn = {1939-1390},
	url = {http://ieeexplore.ieee.org/document/5681215/},
	doi = {10.1109/MITS.2010.939925},
	abstract = {Being able to build a map of the environment and to simultaneously localize within this map is an essential skill for mobile robots navigating in unknown environments in absence of external referencing systems such as {GPS}. This so-called simultaneous localization and mapping ({SLAM}) problem has been one of the most popular research topics in mobile robotics for the last two decades and efﬁcient approaches for solving this task have been proposed. One intuitive way of formulating {SLAM} is to use a graph whose nodes correspond to the poses of the robot at different points in time and whose edges represent constraints between the poses. The latter are obtained from observations of the environment or from movement actions carried out by the robot. Once such a graph is constructed, the map can be computed by ﬁnding the spatial conﬁguration of the nodes that is mostly consistent with the measurements modeled by the edges. In this paper, we provide an introductory description to the graph-based {SLAM} problem. Furthermore, we discuss a state-of-the-art solution that is based on least-squares error minimization and exploits the structure of the {SLAM} problems during optimization. The goal of this tutorial is to enable the reader to implement the proposed methods from scratch.},
	pages = {31--43},
	number = {4},
	journaltitle = {{IEEE} Intelligent Transportation Systems Magazine},
	shortjournal = {{IEEE} Intell. Transport. Syst. Mag.},
	author = {Grisetti, G and Kummerle, R and Stachniss, C and Burgard, W},
	urldate = {2021-02-24},
	date = {2010},
	langid = {english},
	file = {Grisetti et al. - 2010 - A Tutorial on Graph-Based SLAM.pdf:/home/bianca/Zotero/storage/IMZUYD5U/Grisetti et al. - 2010 - A Tutorial on Graph-Based SLAM.pdf:application/pdf},
}